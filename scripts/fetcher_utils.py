# scripts/fetcher_utils.py
import os
import re
import yaml
import requests
import feedparser
from datetime import datetime, timedelta, timezone
from requests.utils import quote

CONFIG_FILE = os.path.join(os.path.dirname(__file__), "..", "config.yaml")
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
README_MD = os.path.join(os.path.dirname(__file__), "..", "README.md")
USER_AGENT = "GenAILabs-ArxivFetcher/1.1"

def get_papers_file(year=None):
    """Get the papers file path for a specific year. If year is None, use current year."""
    if year is None:
        year = datetime.now(timezone.utc).year
    return os.path.join(DATA_DIR, f"papers_{year}.md")

def load_config():
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def fetch_papers(query, start_date=None, end_date=None, max_results=20):
    """
    ë²”ìœ„ ê¸°ë°˜ ê²€ìƒ‰: start_date, end_date (YYYY-MM-DD)
    """
    if start_date:
        start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    else:
        start_dt = datetime.now(timezone.utc) - timedelta(days=7)
    if end_date:
        end_dt = datetime.strptime(end_date, "%Y-%m-%d")
    else:
        end_dt = datetime.now(timezone.utc)

    start_str = start_dt.strftime("%Y%m%d%H%M")
    end_str = end_dt.strftime("%Y%m%d%H%M")

    # arxiv_query = f"all:{topic} AND submittedDate:[{cutoff} TO 30000000000000]"
    # arxiv_query = (
    #     f"(ti:{topic} OR abs:{topic}) AND "
    #     f"submittedDate:[{cutoff} TO 30000000000000]"
    # )

    # ë¶„ì•¼	ì¹´í…Œê³ ë¦¬ ì½”ë“œ
    # ì¸ê³µì§€ëŠ¥	cs.AI
    # ì»´í“¨í„°ë¹„ì „	cs.CV
    # ì–¸ì–´ì²˜ë¦¬	cs.CL
    # ë¨¸ì‹ ëŸ¬ë‹	cs.LG
    # êµìœ¡ê¸°ìˆ 	cs.CY, cs.HC (HCI ê´€ë ¨)
    # ì‹¬ë¦¬/í†µê³„/êµìœ¡	stat.AP, stat.ML

    arxiv_query = (
        f"(ti:{query} OR abs:{query}) "
        f"AND cat:(cs.AI OR cs.CL OR cs.LG OR stat.ML OR cs.CY OR cs.HC) "
        f"AND submittedDate:[{start_str} TO {end_str}]"
    )

    url = (
        "http://export.arxiv.org/api/query?"
        f"search_query={quote(arxiv_query)}"
        f"&sortBy=submittedDate&sortOrder=descending&max_results={max_results}"
    )

    headers = {"User-Agent": USER_AGENT}
    res = requests.get(url, headers=headers, timeout=30)
    res.raise_for_status()
    return feedparser.parse(res.text).entries

def write_papers(entries, name, query, year=None, prepend=False, dry_run=False, date_range=None, is_weekly=False):
    """
    Write papers to year-based file.
    If year is None, papers are grouped by their published year.
    date_range: tuple of (start_date, end_date) for manual searches, displayed in section header
    is_weekly: if True, adds "Weekly Digest" to the section header
    """
    if not entries:
        print(f"â„¹ï¸ No results for {name}")
        return

    # Group entries by year if year is None
    if year is None:
        entries_by_year = {}
        for e in entries:
            pub_date = getattr(e, "published", "")[:10]  # YYYY-MM-DD
            if pub_date:
                pub_year = int(pub_date[:4])
            else:
                pub_year = datetime.now(timezone.utc).year

            if pub_year not in entries_by_year:
                entries_by_year[pub_year] = []
            entries_by_year[pub_year].append(e)

        # Recursively call write_papers for each year
        for year_key, year_entries in entries_by_year.items():
            write_papers(year_entries, name, query, year=year_key, prepend=prepend, dry_run=dry_run, date_range=date_range, is_weekly=is_weekly)
        return

    # Single year processing
    output_file = get_papers_file(year)

    # Determine date display
    if date_range:
        date_display = f"{date_range[0]} ~ {date_range[1]}"
    elif is_weekly:
        date_display = f"{datetime.now(timezone.utc).strftime('%Y-%m-%d')} Weekly Digest"
    else:
        date_display = datetime.now(timezone.utc).strftime("%Y-%m-%d")

    # Always print to console
    print(f"\n{'='*80}")
    print(f"ğŸ“… {date_display} â€” {name}")
    print(f"ğŸ” Query: {query}")
    print(f"ğŸ“Š Found {len(entries)} papers")
    print(f"{'='*80}\n")

    for i, e in enumerate(entries, 1):
        title = re.sub(r"\s+", " ", e.title).strip()
        authors = ", ".join([a.name for a in e.authors])
        pub = getattr(e, "published", "")[:10]
        link = e.link

        print(f"{i}. {title}")
        print(f"   Authors: {authors}")
        if pub:
            print(f"   Published: {pub}")
        print(f"   Link: {link}")
        print()

    # If dry-run, don't write to file
    if dry_run:
        print(f"ğŸ” [DRY RUN] Not writing to file\n")
        return

    # Build markdown section
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    section = f"\n## ğŸ—“ï¸ {date_display} â€” {name}\n<small>{query}</small>\n\n"
    for e in entries:
        title = re.sub(r"\s+", " ", e.title).strip()
        authors = ", ".join([a.name for a in e.authors])
        pub = getattr(e, "published", "")[:10]
        link = e.link
        section += f"**{title}**  \n_Authors:_ {authors}  \n"
        if pub:
            section += f"_Published:_ {pub}  \n"
        section += f"[arXiv Link]({link})\n\n"

    if prepend:
        # Read existing content and prepend new section
        existing_content = ""
        if os.path.exists(output_file):
            with open(output_file, "r", encoding="utf-8") as f:
                existing_content = f.read()

        with open(output_file, "w", encoding="utf-8") as f:
            # Parse header and content
            if existing_content and existing_content.startswith("# "):
                # Find first ## (section start) to separate header from content
                first_section_pos = existing_content.find("\n## ")
                if first_section_pos != -1:
                    header = existing_content[:first_section_pos + 1]  # Include the newline
                    rest_content = existing_content[first_section_pos + 1:]
                else:
                    # No sections yet, entire file is header
                    header = existing_content
                    rest_content = ""

                f.write(header)
                f.write(section)
                f.write(rest_content)
            else:
                # No header exists, create one
                f.write("# ğŸ“š LLM ê¸°ë°˜ êµìœ¡ í‰ê°€ ë…¼ë¬¸ ì•„ì¹´ì´ë¸Œ (ì£¼ê°„ ê°±ì‹ )\n\n")
                f.write(section)
                f.write(existing_content)

        print(f"âœ… Prepended {len(entries)} papers â†’ {output_file}\n")
    else:
        # Append mode (default for weekly updates)
        with open(output_file, "a", encoding="utf-8") as f:
            f.write(section)
        print(f"âœ… Appended {len(entries)} papers â†’ {output_file}\n")
